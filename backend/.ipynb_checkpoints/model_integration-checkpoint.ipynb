{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf03b2cf-5b4d-4b37-8ff4-30010feeceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#Wrapper class for model\n",
    "#Image dimensions standardisation\n",
    "image_dimensions={'height':256,'width':256,'channels':3}\n",
    "\n",
    "class Classifier:\n",
    "    def __init__():\n",
    "        self.model = 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.size == 0:\n",
    "            return []\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "\n",
    "    def get_accuracy(self, x, y):\n",
    "        return self.model.test_on_batch(x, y)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "class Meso4(Classifier):\n",
    "  #Initiialising the class\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(learning_rate = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "    def init_model(self):\n",
    "      #Input Layer\n",
    "        x = Input(shape = (image_dimensions['height'], image_dimensions['width'], image_dimensions['channels']))\n",
    "\n",
    "#4 convolutional blocks\n",
    "        x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "\n",
    "        x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "\n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "\n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "\n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "\n",
    "        return Model(inputs = x, outputs = y)\n",
    "\n",
    "#Initializing model and loading weights\n",
    "model=Meso4()\n",
    "model.load(\"../model_weights/Meso4_DF (1).h5\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image, target_size=(256, 256)):\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the image to match model input\n",
    "    img = cv2.resize(img, target_size)\n",
    "\n",
    "    # Normalize the image\n",
    "    img = img.astype('float32') / 255.0\n",
    "\n",
    "    # Expand dimensions to match model input shape\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    return img\n",
    "\n",
    "# Function to analyze the video and make predictions\n",
    "def analyze_video(video_path, model):\n",
    "    sum_predictions = 0\n",
    "    list_of_predictions = []\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_image(frame)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(preprocessed_frame)\n",
    "\n",
    "        # Display predictions on the frame\n",
    "        prediction_value = float(predictions[0][0])\n",
    "        list_of_predictions.append(prediction_value)\n",
    "\n",
    "\n",
    "        # Display the frame\n",
    "        # cv2.imshow('Video', frame)  # Uncomment to display video with prediction\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Calculate the average of the predictions\n",
    "    for i in list_of_predictions:\n",
    "        sum_predictions+=i\n",
    "   # Check if any predictions were made\n",
    "    if len(list_of_predictions) > 0:\n",
    "        average_prediction = sum_predictions / len(list_of_predictions)\n",
    "    else:\n",
    "        average_prediction = 0  # or handle this case as you prefer\n",
    "    \n",
    "\n",
    "    # Determine if the video is real or fake based on the average prediction\n",
    "    return average_prediction,list(list_of_predictions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4b208f-21a0-4850-8dd1-96d64aac21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def convert_video_to_wav(video_file, output_audio=\"extracted_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Convert a video file to .wav audio format using moviepy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the video file\n",
    "        video = VideoFileClip(video_file)\n",
    "\n",
    "        # Extract audio and write it to a .wav file\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(output_audio, codec='pcm_s16le')  # 'pcm_s16le' for .wav format\n",
    "        print(f\"Audio extracted and saved as {output_audio}\")\n",
    "        return output_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video to audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    \"\"\"\n",
    "    Convert an audio file (wav format) to text using Google Speech Recognition.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Recognizing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "            # Convert speech to text using Google's Speech-to-Text\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(\"Transcribed Text: \", text)\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with Google Speech Recognition service: {e}\")\n",
    "        return None\n",
    "\n",
    "def video_to_text(video_file):\n",
    "    \"\"\"\n",
    "    Complete process: Convert a video file to text by extracting audio and\n",
    "    applying speech-to-text conversion.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the video to wav format\n",
    "    audio_file = convert_video_to_wav(video_file)\n",
    "\n",
    "    if audio_file:\n",
    "        # Step 2: Convert the extracted audio to text\n",
    "        text = audio_to_text(audio_file)\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Audio extraction failed.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0530c89d-1777-4faf-8dbc-ac3ecf469e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 19:34:23.662641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:23.663093: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:23.663747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:23.700554: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-21 19:34:23.709959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:23.710366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:23.710765: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:23.819245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:23.819699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:23.820150: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:23.856569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-21 19:34:23.866714: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:23.867163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:23.867696: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import SpatialDropout3D, Input, Conv3D, BatchNormalization, Activation, MaxPooling3D, Bidirectional, LSTM, Dense, TimeDistributed, ZeroPadding3D, Flatten, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "a=\"\"\n",
    "# Load dLib's pre-trained face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../model_weights/shape_predictor_68_face_landmarks (1).dat\")\n",
    "\n",
    "def load_video(path: str) -> List[float]:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    target_size = (60, 40)  # Define a fixed size for the cropped lip region (height, width)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale (dLib expects grayscale images)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = detector(gray_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            # Get the landmarks/parts for the face\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "\n",
    "            # Extract the coordinates of the lips (points 48-67)\n",
    "            lip_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "\n",
    "            # Create a bounding box around the lips\n",
    "            x_coords, y_coords = zip(*lip_points)\n",
    "            min_x, max_x = min(x_coords), max(x_coords)\n",
    "            min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "            # Crop the lip region from the frame\n",
    "            lip_region = frame[min_y:max_y, min_x:max_x]\n",
    "\n",
    "            # Resize the lip region to the target size\n",
    "            lip_region_resized = cv2.resize(lip_region, target_size)\n",
    "\n",
    "            # Convert to grayscale and append to frames\n",
    "            lip_region_gray = tf.image.rgb_to_grayscale(lip_region_resized)\n",
    "            frames.append(lip_region_gray)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(path: str):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    frames = load_video(path)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def mappable_function(path:str) ->List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "inputs = Input(shape=(75, 40, 60, 1))\n",
    "\n",
    "    # Convolutional layers with BatchNormalization and SpatialDropout3D\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(inputs)\n",
    "x = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(x)\n",
    "x = Activation('relu', name='actv1')(x)\n",
    "x = BatchNormalization(name='batc1')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_1')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(x)\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(x)\n",
    "x = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(x)\n",
    "x = Activation('relu', name='actv2')(x)\n",
    "x = BatchNormalization(name='batc2')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_2')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(x)\n",
    "x = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(x)\n",
    "x = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(x)\n",
    "x = Activation('relu', name='actv3')(x)    \n",
    "x = BatchNormalization(name='batc3')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_3')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(x)\n",
    "\n",
    "    # Reshape for RNN layers\n",
    "x = TimeDistributed(Reshape((-1,)), name='time_distributed_1')(x)\n",
    "    # RNN layers\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal  , name='gru1'), merge_mode='concat')(x)\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal , name='gru2'), merge_mode='concat')(x)\n",
    "\n",
    "    # Dense and Activation layers\n",
    "x = Dense(41, kernel_initializer='he_normal', name='dense1')(x)\n",
    "x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "model_lip = tf.keras.Model(inputs, x)\n",
    "\n",
    "model_lip.load_weights(\"../model_weights/dlib3_lipnet_model (1).h5\")\n",
    "model_lip.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)\n",
    "\n",
    "# Example model prediction line\n",
    "def model_predict(sample):\n",
    "    # Assuming your model expects input of shape (1, 75, 40, 60, 1)\n",
    "    yhat = model_lip.predict(tf.expand_dims(sample, axis=0))\n",
    "    return yhat\n",
    "\n",
    "# Function to split video into chunks of (75, 40, 60, 1)\n",
    "def split_video_into_chunks(video, chunk_size=75):\n",
    "    \"\"\"\n",
    "    Splits the video into chunks of shape (75, 40, 60, 1).\n",
    "    If the last chunk has fewer than 75 frames, it will be padded with zeros.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_frames = video.shape[0]  # The first dimension is the number of frames (x)\n",
    "\n",
    "    # Iterate through the video in steps of 75 frames\n",
    "    for i in range(0, total_frames, chunk_size):\n",
    "        chunk = video[i:i + chunk_size]\n",
    "        \n",
    "        # If the chunk has fewer than 75 frames, pad it\n",
    "        if chunk.shape[0] < chunk_size:\n",
    "            padding = np.zeros((chunk_size - chunk.shape[0], 40, 60, 1))\n",
    "            chunk = np.concatenate((chunk, padding), axis=0)\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Function to process video through the model\n",
    "def process_video_through_model(video):\n",
    "    \"\"\"\n",
    "    Takes a video of shape (x, 40, 60, 1) where x is the number of frames, splits it into\n",
    "    chunks of shape (75, 40, 60, 1), and runs each chunk through the model.\n",
    "    \"\"\"\n",
    "    # Split the video into chunks of (75, 40, 60, 1)\n",
    "    video_chunks = split_video_into_chunks(video)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Run each chunk through the model\n",
    "    for chunk in video_chunks:\n",
    "        print(chunk.shape)\n",
    "        yhat = model_lip.predict(tf.expand_dims(chunk, axis=0))\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "        tensor = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]\n",
    "        string_value = tensor[0].numpy().decode(\"utf-8\")\n",
    "        predictions.append(string_value)\n",
    "    \n",
    "    # Concatenate all predictions into a single string\n",
    "    full_prediction_string = ' '.join(predictions)\n",
    "    \n",
    "    return full_prediction_string\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ea4d03-ad63-4f90-b5e5-e86d98d873e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to track eyelid movement for irregularities\n",
    "def analyze_blink_smoothness(ear_values, threshold=0.05):\n",
    "    differences = np.diff(ear_values)  # Get the differences between consecutive EAR values\n",
    "    irregular_movements = sum(abs(diff) > threshold for diff in differences)  # Count large differences\n",
    "    return irregular_movements\n",
    "\n",
    "# Main function to process the video and return results\n",
    "def process_video(video_path):\n",
    "    # Load the pre-trained facial landmark detector and shape predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"../model_weights/shape_predictor_68_face_landmarks (1).dat\")\n",
    "\n",
    "    # Eye aspect ratio threshold for blink detection\n",
    "    EYE_AR_THRESH = 0.25\n",
    "    EYE_AR_CONSEC_FRAMES = 3\n",
    "    BLINK_IRREGULARITY_THRESH = 3  # Customize based on experiments\n",
    "\n",
    "    # Initialize counters and variables\n",
    "    blink_counter = 0\n",
    "    total_blinks = 0\n",
    "    irregular_blinks = 0\n",
    "    blink_started = False\n",
    "    ear_values = []\n",
    "\n",
    "    # Start video capture\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Grab the indexes of the facial landmarks for the left and right eye\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    frame_to_return = None  # Initialize the frame to return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=450)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # Visualize the landmarks for the eyes by drawing circles\n",
    "            for (x, y) in leftEye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Draw circles for the left eye\n",
    "            for (x, y) in rightEye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Draw circles for the right eye\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                blink_counter += 1\n",
    "                ear_values.append(ear)  # Track EAR during blink\n",
    "                if not blink_started:\n",
    "                    blink_started = True\n",
    "                    ear_values = [ear]  # Start tracking from the first frame of blink\n",
    "            else:\n",
    "                if blink_counter >= EYE_AR_CONSEC_FRAMES:\n",
    "                    total_blinks += 1\n",
    "\n",
    "                    # Check for irregularities in blink smoothness\n",
    "                    irregularities = analyze_blink_smoothness(ear_values)\n",
    "                    if irregularities > BLINK_IRREGULARITY_THRESH:\n",
    "                        irregular_blinks += 1\n",
    "\n",
    "                blink_counter = 0\n",
    "                blink_started = False\n",
    "\n",
    "        # Store the current frame for returning later\n",
    "        frame_to_return = frame\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "    # If a frame was processed, convert it to base64\n",
    "    if frame_to_return is not None:\n",
    "        # Add text for total blinks and irregular blinks to the frame\n",
    "        cv2.putText(frame_to_return, f\"Total Blinks: {total_blinks}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame_to_return, f\"Irregular Blinks: {irregular_blinks}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Convert the frame to a PIL image and then to a byte buffer\n",
    "        _, buffer = cv2.imencode('.png', frame_to_return)\n",
    "        pil_image = Image.open(io.BytesIO(buffer))\n",
    "        \n",
    "        img_buffer = io.BytesIO()\n",
    "        pil_image.save(img_buffer, format=\"PNG\")\n",
    "        img_buffer.seek(0)\n",
    "        \n",
    "        # Encode the image as base64 for return or transmission\n",
    "        img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "        # Return the base64 image and blink information\n",
    "        return {\"image_base64\": img_base64, \"total_blinks\": total_blinks, \"irregular_blinks\": irregular_blinks}\n",
    "    else:\n",
    "        # Return default values if no frame was processed\n",
    "        return {\"image_base64\": None, \"total_blinks\": total_blinks, \"irregular_blinks\": irregular_blinks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fad5b5-20c6-4b51-9a8a-1ccec2c5bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image dimensions standardisation\n",
    "image_dimensions={'height':256,'width':256,'channels':3}\n",
    "\n",
    "def extract_mfcc_features(audio_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    try:\n",
    "        audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "   \n",
    "\n",
    "def analyze_audio(input_audio_path):\n",
    "    scaler_filename = \"../model_weights/scaler (1).pkl\"\n",
    "    model_filename = \"../model_weights/svm_model_best (1).pkl\"\n",
    "    svm_classifier = joblib.load(model_filename)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    if not os.path.exists(input_audio_path):\n",
    "        print(\"Error: The specified file does not exist.\")\n",
    "        return\n",
    "    elif not input_audio_path.lower().endswith(\".wav\"):\n",
    "        print(\"Error: The specified file is not a .wav file.\")\n",
    "        return\n",
    "\n",
    "    mfcc_features = extract_mfcc_features(input_audio_path)\n",
    "\n",
    "    if mfcc_features is not None:\n",
    "        mfcc_features_scaled = scaler.transform(mfcc_features.reshape(1, -1))\n",
    "        prediction = svm_classifier.predict(mfcc_features_scaled)\n",
    "        if prediction[0] == 0:\n",
    "            return \"The input audio is classified as genuine.\"\n",
    "        else:\n",
    "            return \"The input audio is classified as deepfake.\" \n",
    "    else:\n",
    "        return \"Error: Unable to process the input audio.\"\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Check if each class has at least two samples\n",
    "    if len(X_genuine) < 2 or len(X_deepfake) < 2:\n",
    "        print(\"Each class should have at least two samples for stratified splitting.\")\n",
    "        print(\"Combining both classes into one for training.\")\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "    else:\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c373804-8252-431a-a9df-a6739e32dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "\n",
      "INFO:     Started server process [62360]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 19:34:39.580996: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "(75, 40, 60, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 19:34:47.439153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:47.439998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:47.440561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:47.491829: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-21 19:34:47.504393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:47.504869: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:47.505434: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:47.567294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:47.567802: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:47.568347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-21 19:34:47.616949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-21 19:34:47.629473: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-21 19:34:47.630009: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-21 19:34:47.630490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 462ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Error converting video to audio: 'NoneType' object has no attribute 'write_audiofile'\n",
      "Audio extraction failed.\n",
      "INFO:     127.0.0.1:59342 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59369 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "import os\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "UPLOAD_DIRECTORY = \"./uploaded_files\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3002\"], \n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload_file(video: UploadFile = File(None), audio: UploadFile = File(None)):\n",
    "    result = None\n",
    "    random_array = []\n",
    "    micro=None\n",
    "    frame_base64=None\n",
    "    dct_base64=None\n",
    "    image_base64=None\n",
    "    total_blinks=None\n",
    "    irregular_blinks=None\n",
    "    full_prediction_string=None\n",
    "    transcribed_text = None\n",
    "    similarity=None\n",
    "    gaze=None\n",
    "    lip=None\n",
    "    mfcc1=None\n",
    "    mfcc2=None\n",
    "    mfcc3=None\n",
    "    mfcc1_64=None\n",
    "    mfcc2_64=None\n",
    "    prediction=None\n",
    "    mfcc3_64=None\n",
    "    final_result=None\n",
    "    result1=None\n",
    " \n",
    "   \n",
    "    \n",
    "    if video:\n",
    "        video_path = os.path.join(UPLOAD_DIRECTORY, video.filename)\n",
    "        with open(video_path, \"wb\") as buffer:\n",
    "            buffer.write(await video.read())\n",
    "\n",
    "        # Call the video analysis function\n",
    "     \n",
    "        result, random_array = analyze_video(video_path,model)\n",
    "        if result>0.5:\n",
    "            micro=\"The Microexpressions are human-like and this is not a deepfake.\"\n",
    "        else:\n",
    "            micro=\"The Microexpressions are not human-like and this is a deepfake.\"\n",
    "\n",
    "       \n",
    "\n",
    "        eye_result=process_video(video_path)\n",
    "        image_base64 = eye_result[\"image_base64\"]\n",
    "        total_blinks = eye_result[\"total_blinks\"]\n",
    "        irregular_blinks = eye_result[\"irregular_blinks\"]\n",
    "\n",
    "        if irregular_blinks>1:\n",
    "            gaze=\"The blinks are irregular which suggest that it is a deepfake.\"\n",
    "        else:\n",
    "            gaze=\"The blinks are regular which suggests that it is not a deepfake\"\n",
    "\n",
    "        video = load_data(tf.convert_to_tensor(video_path))\n",
    "        full_prediction_string = process_video_through_model(video)\n",
    "\n",
    "        transcribed_text = video_to_text(video_path)\n",
    "        similarity = fuzz.ratio(full_prediction_string, transcribed_text)\n",
    "        if similarity>50:\n",
    "            lip=\"The lip-synchnorisation Score is high enough for it to be not a deepfake.\"\n",
    "        elif similarity==0:\n",
    "            lip=\"There is no audio present\"\n",
    "        else:\n",
    "            lip=\"The lip-synchronisation Score is low so it is deepfake\"\n",
    "\n",
    "        if similarity<60 or result<0.5 or irregular_blinks>1:\n",
    "            final_result=\"Final Conclusion:This is a deepfake.\"\n",
    "        else:\n",
    "            final_result=\"Final Conclusion:This is not a deepfake\"\n",
    "\n",
    "    if audio:\n",
    "        audio_path = os.path.join(UPLOAD_DIRECTORY, audio.filename)\n",
    "        with open(audio_path, \"wb\") as buffer:\n",
    "            buffer.write(await audio.read())\n",
    "        # Create in-memory buffers for images\n",
    "         # Load audio and plot data\n",
    "        real_ad, real_sr = librosa.load(audio_path)\n",
    "\n",
    "        # Create in-memory buffers for images\n",
    "        mfcc1 = io.BytesIO()\n",
    "        mfcc2 = io.BytesIO()\n",
    "        mfcc3 = io.BytesIO()\n",
    "\n",
    "        # Plot waveform and save as image\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(real_ad)\n",
    "        plt.title(\"Audio Data\")\n",
    "        plt.tight_layout()  # Ensure the layout is tight to avoid clipping\n",
    "        plt.savefig(mfcc1, format='png', bbox_inches='tight')  # Save plot to buffer\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc1.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Plot Mel spectrogram\n",
    "        real_mel_spect = librosa.feature.melspectrogram(y=real_ad, sr=real_sr)\n",
    "        real_mel_spect = librosa.power_to_db(real_mel_spect, ref=np.max)\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(real_mel_spect, y_axis=\"mel\", x_axis=\"time\")\n",
    "        plt.title(\"Audio Mel Spectrogram\")\n",
    "        plt.colorbar(format=\"%+2.0f dB\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(mfcc2, format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc2.seek(0)\n",
    "\n",
    "        # Plot MFCCs\n",
    "        real_mfccs = librosa.feature.mfcc(y=real_ad, sr=real_sr)\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(real_mfccs, sr=real_sr, x_axis=\"time\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Audio MFCCs\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(mfcc3, format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc3.seek(0)\n",
    "\n",
    "        # Convert to base64 strings\n",
    "        mfcc1_64 = base64.b64encode(mfcc1.read()).decode('utf-8')\n",
    "        mfcc2_64 = base64.b64encode(mfcc2.read()).decode('utf-8')\n",
    "        mfcc3_64 = base64.b64encode(mfcc3.read()).decode('utf-8')\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "      \n",
    "\n",
    "       \n",
    "        result1 = analyze_audio(audio_path)  # Single value result for audio\n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "    return {\"result\": result, \"random_array\": random_array,  \"prediction\": prediction,\n",
    "            \"frame_base64\": frame_base64,\n",
    "            \"dct_base64\": dct_base64,\"image_base64\":image_base64,\"total_blinks\":total_blinks,\"irregular_blinks\":irregular_blinks,\"full_prediction_string\":full_prediction_string,\"transcribed_text\": transcribed_text,\"similarity\":similarity ,\"micro\":micro,\n",
    "           \"gaze\":gaze,\"lip\":lip,\"mfcc1_64\":mfcc1_64,\"mfcc2_64\":mfcc2_64,\"mfcc3_64\":mfcc3_64,\"final_result\":final_result,\"result1\":result1}\n",
    "\n",
    "# Run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ca59f-de61-4af6-8754-5edeb2b49c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce217f-8c6c-437e-a5fd-a7f62b3c04ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1435154-af51-48b5-b601-cfc4f2c3531c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
